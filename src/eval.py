"""
Code based on:
https://github.com/cenguix/Text2KGBench/blob/main/src/evaluation/run_eval.py

Adapted for MaintIE2KGBench suitability
"""

import argparse
import os
import json
import re

import pandas as pd
from typing import List, Dict, Set, Tuple


def calculate_precision_recall_f1(gold: Set, pred: Set) -> (float, float, float):
    """
    Method to calculate precision, recall and f1:
        Precision is calculated as correct_triples/predicted_triples and
        Recall as correct_triples/gold_triples 
        F1 as the harmonic mean of precision and recall.
    :param gold: items in the gold standard
    :param pred: items in the system prediction
    :return:
        p: float - precision
        r: float - recall
        f1: float - F1
    """
    if len(pred) == 0:
        return 0, 0, 0
    p = len(gold.intersection(pred)) / len(pred)

    if len(gold) == 0: #Some texts dont have any triples, thus recall will be 0 by default for anything retrieved.
        r = 0
    else:
        r = len(gold.intersection(pred)) / len(gold)
    if p + r > 0:
        f1 = 2 * ((p * r) / (p + r))
    else:
        f1 = 0
    return p, r, f1


def get_subject_object_hallucinations(ontology, test_sentence, triples) -> (float, float):
    """
    Calculate subject and object hallucinations metrics. As the context for calculating hallucinations, we consider the
    test sentence and the ontology concepts as relevant tokens.
    :param ps: stemmer for stemming words before checking for hallucinations
    :param ontology: ontology to take into account with the concepts and relations
    :param test_sentence: test sentences for which the triples are generated
    :param triples: a set of triples generated by the system
    :return:
        subj_hallucination: float - subject hallucination metric
        obj_hallucination: float - object hallucination metric
    """
    # if the set of triples are empty, we return 0
    if len(triples) == 0:
        return 0, 0

    # append the test sentence with concepts from the ontology
    test_sentence += " ".join([c["label"] for c in ontology['concepts']])

    # count the number of subject and object hallucinations
    num_subj_hallucinations, num_obj_hallucinations = 0, 0
    for triple in triples:
        subject = triple[1]
        object = triple[2]
        
        # check if the subject/object is found in the sentence/context text. If not found, mark it as a hallucination
        if test_sentence.find(object) == -1:
            num_obj_hallucinations += 1        
        if test_sentence.find(subject) == -1:
            num_subj_hallucinations += 1
    # divide the number of hallucinations by the number of triples to calculate the hallucination metrics
    subj_hallucination = num_subj_hallucinations / len(triples)
    obj_hallucination = num_obj_hallucinations / len(triples)
    return subj_hallucination, obj_hallucination


def get_ontology_conformance(relations: List, triples: List) -> (float, float):
    """
    Calculate the ontology conformance and relation hallucination metrics.
    :param ontology: ontology to take into account with the concepts and relations
    :param triples: a set of triples generated by the system
    :return:
        ont_conformance: float - ontology conformance metric
        rel_hallucination: float - relation hallucination metric = 1 - ontology conformance
    """
    if len(triples) == 0:
        return 1, 0
    
    # count the number of system triples relations that are in the ontology
    num_rels_conformant = len([tr for tr in triples if tr[0] in relations])

    # ontology conformance is the number of system triples relations in the ontology divided by the total number of system triples
    ont_conformance = num_rels_conformant / len(triples)
    # relation hallucination is 1 - ontology conformance
    rel_hallucination = 1 - ont_conformance
    return ont_conformance, rel_hallucination


def extract_triples(llm_response):
    """
    Detects triples of the form r(a,b)
    """
    triples = []
    for item in llm_response.split('\n'):
        matches = re.findall("(\w+)\(([^\),]+),([^\)]+)\)", item)
        for triple in matches:
            triples.append(triple)
    return triples

def extract_filtered_triples(entry, ontology):
    """
    Detects triples of the form r(a,b)
    """
    triples = []
    ontology_relations = [relation['relations'] for relation in ontology['relations']]
    sentence = entry['target']
    for item in entry['generated_text'].split('\n'):
        matches = re.findall("(\w+)\(([^\),]+),([^\)]+)\)", item)
        if len(matches) == 0:
            continue
        for triple in matches:
            relation = triple[0]
            subject = triple[1]
            object = triple[2]

            #print(sentence, triple, relation not in ontology_relations, sentence.find(object), sentence.find(subject))
            if relation not in ontology_relations or sentence.find(object) == -1 or sentence.find(subject) == -1:
                continue
            else:
                triples.append(triple)
            break
    return triples


def main(data_file, filter=False):

    with open(data_file, 'r') as f:
        data = [json.loads(line) for line in f]

    with open(args.ontology) as in_file:
        ontology = json.load(in_file)

    
    #gt_concepts = [concept['label'] for concept in ontology['concepts']]
    gt_relations = [relation['relations'] for relation in ontology['relations']]
        # initialize the local variables for the evaluation metrics for each ontology
    t_p, t_r, t_f1, t_onto_conf, t_rel_halluc, t_sub_halluc, t_obj_halluc = 0, 0, 0, 0, 0, 0, 0
  
    # iterate through each element in the ground truth and evaluate the system output
    for entry in data:
        sentence = entry['target']
        gt_triples = extract_triples(entry['target_answer'])
        if filter == False:
            predictions = extract_triples(entry['generated_text'])
        else:
            #Filter any triple where the relation is not in ontology, or where object/subject isnt in the sentence
            predictions = extract_filtered_triples(entry, ontology)

        # compare the system output triples with ground truth triples and calculate precision, recall, f1
        precision, recall, f1 = calculate_precision_recall_f1(set(gt_triples), set(predictions))
        # calculate ontology conformance and relation hallucination
        ont_conformance, rel_hallucination = get_ontology_conformance(gt_relations, predictions)

        # calculate subject and object hallucination
        subj_hallucination, obj_hallucination = get_subject_object_hallucinations(ontology, sentence, predictions)
        #if  f1 < 1  and len(filtered_predictions) > 0 and subj_hallucination == 0 and obj_hallucination == 0:
            #print(f"sent: {sentence}\nf1: {f1}\nsys:{filtered_predictions}\nground:{gt_triples}\n\n")

        # aggregate precision, recall, f1 for later averaging
        t_p += precision
        t_r += recall
        t_f1 += f1
        t_onto_conf += ont_conformance
        t_rel_halluc += rel_hallucination
        t_sub_halluc += subj_hallucination
        t_obj_halluc += obj_hallucination

    total_test_cases = len(data)
    # average metrics calculate the average of evaluate metrics for all test cases in a given ontology
    average_metrics = {"avg_precision": f"{t_p/total_test_cases:.2f}",
                       "avg_recall": f"{t_r/total_test_cases:.2f}",
                       "avg_f1": f"{t_f1/total_test_cases:.2f}",
                       "avg_onto_conf": f"{t_onto_conf/total_test_cases:.2f}",
                       "avg_sub_halluc": f"{t_sub_halluc/total_test_cases:.2f}",
                       "avg_rel_halluc": f"{t_rel_halluc / total_test_cases:.2f}",
                       "avg_obj_halluc": f"{t_obj_halluc/total_test_cases:.2f}"
                        }
    print(data_file, average_metrics)
    return average_metrics

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # please have a look at src/evaluation/config for examples of evaluation configs.
    parser.add_argument('--results_file', type=str, default="./results.csv")
    parser.add_argument('--prediction_folder', type=str, default="./predictions/")
    parser.add_argument('--ontology', type=str, default="./data/maintie_silver_ontology.json")
    parser.add_argument('--filter_hallucinations', action="store_true") 
    
    args = parser.parse_args()

    list_of_results = []
    for root, subdirs, files in os.walk(args.prediction_folder):
        for file in sorted(files):
            output = main(os.path.join(root, file), args.filter_hallucinations)
            output['model'] = root
            output['file'] = file
            list_of_results.append(output)

    df = pd.DataFrame(list_of_results)
    df.to_csv(args.results_file, index=False)
    print(df)

